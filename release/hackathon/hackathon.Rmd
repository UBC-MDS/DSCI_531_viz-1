---
title: "DSCI 531: Hackathon"
output: github_document
---

Due Tuesday, Nov 6 at 18:00 (although you should be able to finish in lecture)

```{r}
suppressPackageStartupMessages(library(tidyverse))
```

# Objective

In this course, we've looked at four layers of data vis:

1. Drawing conclusions from visualizations
    - Guiding question: what can we see in this graph that we can't see by looking at a table of the raw data?
2. Using tools to create a graph
    - `ggplot2` and other niche R packages; also `matplotlib`, `pandas`, and `seaborn` in Python.
3. Components of a graph: the grammar of graphics, and theme elements.
4. Effective graph choice

In this "half lab" assignment, you create a graph in a team of 2-3 that touches on all of these components, by choosing one of five analysis scenarios. Your submission will include the following parts:

1. A graph.
2. A figure caption.
3. A description of your design choice.

The following section outlines the evaluation of this assignment. The scenarios follow.

# Evaluation

## Tidy Submission (worth 5%)

rubric={mechanics:5}

To get the marks for tidy submission:

- __INDICATE WHO YOUR HACKATHON TEAM MEMBERS ARE__! (Both names and github.ubc.ca usernames)
- This document only contains instructions. Where you put your work is up to you, but it's important for you to make it obvious where your work can be found!
- Use either jupyter notebook or R Markdown.
- Use either R or Python.
- Be sure to follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions/).
- Do not include any code that installs packages (this is not good practice anyway).

## Writing (worth 5%)

rubric={writing:5}

To get these marks, you must use proper English, spelling, and grammar in your submission.

## Publication Quality (worth 10%)

rubric={vis:10}

To get these marks, your graph must be publication quality.

## Design choice (worth 35%)

rubric={reasoning:40}

To get these marks, briefly describe the decisions you made in designing this graph to be effective. Don't write a lot. Only focus on the big picture things here. Point form is fine.

## Code accuracy and quality (worth 30%)

rubric={accuracy:15, quality:15}

To get these marks, your code must work (`accuracy` rubric) and must be high quality (`quality` rubric: readable and reasonably efficient).

## Figure caption (worth 15%)

To get these marks, you should accompany your graph with a figure caption that:

1. Orients the reader to the graph (like a graph title would).
    - Example: "Relationship between GDP per capita and life expectancy"
2. Fills readers in with critical information that the graph could not convey. This might not be applicable, but it probably will be.
    - Example: "taken every 5 years between 1952 and 2007"
3. Draws a conclusion from the graph.
    - Example: "countries with higher GDP per capita tend to have a higher life expectancy."

# Scenarios

## Scenario A

Every day, you run a model that predicts tomorrow's river discharge of the Bow River at Banff. On some days, the model produces an invalid forecast (this makes sense in the context of probabilistic forecasts, something you'll learn in DSCI 562). You suspect that invalid forecasts (called `error` in the data) are more likely to happen when the actual discharge that materializes the next day (called `outcome` in the data) is larger. Your task is to produce a visual to explore this, while still giving the viewer a sense of the overall chance of an invalid forecast.

```{r}
(sA <- read_csv("data/sA-error_prop.csv"))
```


## Scenario B

Suppose you have snowmelt (in millimeters) and river discharge (in m^3/s) data that are recorded daily, although some days have missing data. You'd like to show your audience _when_ you have data on record, for both variables.

The data are given in two forms for your data wrangling convenience: 

- In `sBa`, each row corresponds to an available observation of data type `Type` on date `date`. 
- In `sBb`, each row corresponds to an (inclusive) range of dates where data are available. 


```{r}
(sBa <- read_csv("data/sB-time_intervals-pointwise.csv"))
(sBb <- read_csv("data/sB-time_intervals-range.csv"))
```


## Scenario C

Suppose you're writing a report to justify a prediction model you've built, which predicts river discharge (in m^3/s) using snowmelt (in millimeters). Part of your investigation involves trying out different (increasing) weight functions on snowmelt (you'll learn more about weight functions later in the program, but their meaning is not important here). You want to convince your readers that your choice of weight functions do a fairly good job spanning the choices of sensible weight functions.

In other words, the weight functions should increase from 0 to 1 at different rates and at different positions, and this increase should happen in the range of sensible snowmelt values (snowmelt data are recorded in `sC-weight_justif.csv`).

You've chosen nine weight functions. They're stored in the list `wfun` below. (If you're interested, they are logistic functions with all combinations of three choices of the location parameter `x0`, and three choices of the rate parameter `r`. This information does not help you jusify your prediction model in this imaginary report.)

```{r}
(sC <- read_csv("data/sC-weight_justif.csv"))
## Define weight functions
logistic <- function(x0, r) function(x) 1 / (1 + exp(-r*(x-x0)))
wfun <- crossing(
    x0 = c(0.0, 7.5, 15.0),
    r  = c(0.05, 0.20, 0.70)
) %>% 
    mutate(f = map2(x0, r, logistic)) %>% 
    `[[`("f")
```


## Scenario D

You have daily snowmelt data (in millimeters) at a particular location near Canmore, Alberta. You notice that the distribution of snowmelt on a given day changes across the year, so you've estimated the distribution for every day of the year.

Someone's given you many years worth of new data. Make a graph to show whether the new data "agrees" with your distribution estimates -- that is, whether or not the data could have plausibly been drawn from the distributions you estimated. It would also be useful to show that your distribution estimates match up with the original data, too.

About the data:

- `sD-dist_melt.csv` has the snowmelt data, new and original.
- `sD-dist_melt.Rdata` (too large to gift: find in students repo, or click [here](https://github.ubc.ca/MDS-2018-19/DSCI_531_viz-1_students/raw/master/release/hackathon/data/sD-dist_melt.Rdata)) loads a list with variable name `dist_melt` containing the distributions. Density estimates are not available, but the cdf and quantile functions are, and are stored as list elements 1 and 2, respectively.
    - cdf arguments:
        - `x`: vectorized. The variable of the distribution.
        - `d`: day of the year; integer acceptable from 1 to 366.
    - quantile function arguments:
        - `p`: vectorized. Probabilities to find quantiles of.
        - `d`: day of the year; integer acceptable from 1 to 366. 


```{r}
(sF <- read_csv("data/sD-dist_melt.csv"))
load("data/sD-dist_melt.Rdata")
str(dist_melt)
```
